(this["webpackJsonpai-quiz"]=this["webpackJsonpai-quiz"]||[]).push([[0],{243:function(e,a,t){e.exports=t(350)},248:function(e,a,t){},249:function(e,a,t){},254:function(e,a,t){},255:function(e,a,t){},256:function(e,a,t){},257:function(e,a,t){},279:function(e,a,t){},350:function(e,a,t){"use strict";t.r(a);var i=t(0),n=t.n(i),l=t(8),o=t.n(l),s=(t(248),t(249),t(94)),r=t(14),c=(t(254),t(363)),d=(t(255),t(236)),u=t(16),h=t(362),b=(t(256),t(93)),p=t.n(b),m=t(6),v=t(369),f=(t(257),{SET_CHECKBOX:"SET_CHECKBOX",SET_SUBMITTED:"SET_SUBMITTED"}),g=Object(m.a)({root:{color:u.a.green[400],"&$checked":{color:u.a.green[900]}},checked:{}})((function(e){return n.a.createElement(v.a,Object.assign({color:"default"},e))})),w=Object(m.a)({root:{color:u.a.red[400],"&$checked":{color:u.a.red[900]}},checked:{}})((function(e){return n.a.createElement(v.a,Object.assign({color:"default"},e))})),y=Object(r.b)((function(e){return{submitted:e.quiz.submitted}}),(function(e){return{setCheckbox:function(a,t,i){return e(function(e,a,t){return{type:f.SET_CHECKBOX,payload:{checked:e,quizIdx:a,optionIdx:t}}}(a,t,i))}}}))((function(e){var a=e.option,t=e.quizIdx,i=e.optionIdx,l=e.setCheckbox,o=e.submitted;return o&&a.checked^a.valid?n.a.createElement(w,{checked:!!a.checked&&a.checked,disabled:o,onChange:function(e){var a=e.target.checked;l(a,t,i)}}):!o||a.checked^a.valid?n.a.createElement(v.a,{checked:!!a.checked&&a.checked,disabled:o,onChange:function(e){var a=e.target.checked;l(a,t,i)},color:"primary"}):n.a.createElement(g,{checked:!!a.checked&&a.checked,disabled:o,onChange:function(e){var a=e.target.checked;l(a,t,i)}})})),k=p()({root:{color:u.a.green[900]}})((function(e){return n.a.createElement(h.a,e)})),x=p()({root:{color:u.a.red[900]}})((function(e){return n.a.createElement(h.a,e)})),E=Object(r.b)((function(e){return{submitted:e.quiz.submitted}}))((function(e){var a=e.submitted,t=e.option,i=Object(d.a)(e,["submitted","option"]);return a&&t.checked^t.valid?n.a.createElement(x,{control:n.a.createElement(y,Object.assign({option:t},i)),label:t.label}):!a||t.checked^t.valid?n.a.createElement(h.a,{control:n.a.createElement(y,Object.assign({option:t},i)),label:t.label}):n.a.createElement(k,{control:n.a.createElement(y,Object.assign({option:t},i)),label:t.label})})),I=Object(r.b)((function(e){return{submitted:e.quiz.submitted}}))((function(e){var a=e.quiz,t=e.quizIdx,i=e.submitted;return n.a.createElement("div",{className:"question"},n.a.createElement(c.a,null,n.a.createElement("p",null,a.question),a.responses.map((function(e,a){return n.a.createElement("div",{className:"option",key:a},n.a.createElement(E,{option:e,quizIdx:t,optionIdx:a}),e.explication&&i&&e.checked&&!e.valid?n.a.createElement("div",{className:"explication"},e.explication):null,i&&!e.checked&&e.valid?n.a.createElement("div",{className:"explication"},"This should be selected !"):null)}))))})),q=t(364),T=(t(279),function(e){var a=e.handleSubmit,t=e.disabled;return n.a.createElement(q.a,{variant:"contained",color:"primary",fullWidth:!1,onClick:a,disabled:t},"Submit")}),z=t(365),C=t(368),S=t(367),O=t(356),A=Object(z.a)((function(e){return{modal:{display:"flex",alignItems:"center",justifyContent:"center"},paper:{backgroundColor:e.palette.background.paper,border:"2px solid #000",boxShadow:e.shadows[5],padding:e.spacing(2,4,3)}}})),N=function(e){var a=e.open,t=e.handleClose,i=e.score,l=A();return n.a.createElement("div",null,n.a.createElement(C.a,{"aria-labelledby":"transition-modal-title","aria-describedby":"transition-modal-description",className:l.modal,open:a,onClose:t,closeAfterTransition:!0,BackdropComponent:S.a,BackdropProps:{timeout:500}},n.a.createElement(O.a,{in:a},n.a.createElement("div",{className:l.paper},n.a.createElement("h2",{id:"transition-modal-title"},"Your score is:"),n.a.createElement("p",{id:"transition-modal-description"},Number.parseFloat(i).toFixed(2),"%")))))},j=Object(r.b)((function(e){return{quizes:e.quiz.quizes,submitted:e.quiz.submitted}}),(function(e){return{setSubmitted:function(a){return e(function(e){return{type:f.SET_SUBMITTED,payload:{submitted:e}}}(a))}}}))((function(e){var a=e.quizes,t=e.submitted,i=e.setSubmitted,l=n.a.useState(!1),o=Object(s.a)(l,2),r=o[0],c=o[1],d=n.a.useState(0),u=Object(s.a)(d,2),h=u[0],b=u[1];return n.a.createElement("div",{className:"quiz"},a.map((function(e,a){return n.a.createElement(I,{quiz:e,key:a,quizIdx:a})})),n.a.createElement("div",{className:"button"},n.a.createElement(T,{handleSubmit:function(){c(!0),i(!0);var e=a.reduce((function(e,a){return e+(a.responses.reduce((function(e,a){return a.checked===a.valid&&e}),!0)?1:0)}),0);b(e/a.length)},disabled:t})),n.a.createElement(N,{open:r,handleClose:function(){c(!1)},score:h}))}));var W=function(){return n.a.createElement("div",{className:"App"},n.a.createElement("h2",null,"AI Quiz"),n.a.createElement(j,null))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));var M=t(25),B=t(95),D=t(235),L=t.n(D),P={quizes:[{question:"1.\tWhat is true about deductive reasoning:",responses:[{label:"It uses a top-down approach",valid:!0,checked:!1},{label:"the conclusions are probabilistic",valid:!1,explication:"This is inductive reasoning. In deductive, reasoning uses available facts, information, or knowledge to assume a valid conclusion",checked:!1},{label:"It moves from generalized statement to an effective conclusion ",valid:!0,checked:!1},{label:"fast and easy, as we need evidence instead of true facts",valid:!1,explication:"Wrong. In contrast, it is difficult because we need facts to be true",checked:!1}]},{question:"2.\tUnivariate, bivariate and multivariate data differ in:",responses:[{label:"The number of dependent variables",valid:!0,checked:!1},{label:"How to deal with causes and relationships",valid:!1,explication:"Wrong, because in univariate data we have one variable. Our main purpose is to describe data and find patterns",checked:!1},{label:"How to perform analysis in data",valid:!0,explication:"That\u2019s right. The ways to perform analysis in data depends on the goals we want to achieve",checked:!1}]},{question:"3. What is skewed data:",responses:[{label:"Normal distribution of data",valid:!1,checked:!1},{label:"Not symmetric distribution",valid:!0,checked:!1},{label:"Any data that contains outliers",valid:!1,checked:!1}]},{question:"4. Data wrangling:",responses:[{label:"Known as data Munging",valid:!0,checked:!1},{label:"Convert raw data to an appropriate form to advanced tasks",valid:!0,checked:!1},{label:"Last step in Data preprocessing",valid:!0,checked:!1},{label:"Convert raw data to a clean form",valid:!1,explication:"Wrong. This is preprocessing",checked:!1},{label:"Convert data from a given form to more usable and informative form",valid:!1,explication:"Wrong, this is Processing of data",checked:!1}]},{question:"5. Independent variable in a dataset:",responses:[{label:"Feature",valid:!0,checked:!1},{label:"Input",valid:!0,checked:!1},{label:"Label",valid:!1,explication:"Wrong. This is called dependent variable",checked:!1},{label:"Outcome",valid:!1,explication:"Wrong. This is called dependent variable",checked:!1}]},{question:"6. Methods to detect and remove noise in a dataset:",responses:[{label:"K-fold validation",valid:!0,checked:!1},{label:"One class SVM",valid:!0,checked:!1},{label:"Neural network",valid:!0,checked:!1},{label:"Auto encoder",valid:!0,checked:!1},{label:"Clustering",valid:!0,checked:!1},{label:"Density-based algorithms",valid:!0,checked:!1}]},{question:"7. How can we handle missing data effectively:",responses:[{label:"Always delete data rows with missing values ",valid:!1,explication:"We can\u2019t do this all the time because we will lose a lot of information; it is possible only when percentage of missing values is (<5%) or almost values in the row are missing",checked:!1},{label:"Drop column with missing data and pretend that the variable does not existed",valid:!0,explication:"Right in condition that 15% of data is missing in that column",checked:!1},{label:"Replace the missing value with the previous or next value",valid:!1,explication:"This does not solve the problem, because we risk to have missing value also in the next or previous value",checked:!1}]},{question:"8.\tData Leakage is",responses:[{label:"Less data",valid:!1},{label:"The training data contains information about the target which will not be available when predicting.",valid:!0},{label:" It leads to poor performances in on the training set, but high prediction performances",valid:!1,explication:"In contrary"},{label:"Not carefully distinguish training data from validation data",valid:!0},{label:"Variables updated (or created) after the target value is realized is included",valid:!1,explication:"To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded."}]},{question:"9.\tWhat is true about the following statements",responses:[{label:" If the data is skewed then, missing values can be imputed or replaced by mean of the all observations of the dataset",valid:!1,explication:"This is the case for normally distributed data."},{label:"If the data is skewed then, it is better to impute or replace the missing values by Median of all observations of the dataset.",valid:!0},{label:"Median is the middle value in a dataset",valid:!0,explication:"Alright. Mean is the average of a dataset, and mode is the most frequently observation in a dataset"}]},{question:"10.\tWhat are dummy variables",responses:[{label:"New variables created from categories presented in categorical variable",valid:!0},{label:"Variables created to transform categorical data to numbers",valid:!0},{label:"Independent variables",valid:!1,explication:" They could be applied to both dependent and indepedent variables"},{label:"Created using one hot encoding",valid:!0}]},{question:"11.\tData Binning",responses:[{label:" Transform categorical variables into numerical",valid:!1,explication:"In  contrary"},{label:"Data processing method",valid:!1,explication:" It is data preprocessing"},{label:"Quantization technique to handle continuous variables",valid:!0},{label:"Fixed-width binning is based on domain knowledge to create fix width bins",valid:!0},{label:"In fixed-width binning, data distribution decides bin ranges for itself",valid:!1,explication:"Wrong. This is called adaptive binning"}]},{question:"12.\tWhat is true about normalization",responses:[{label:"Normalization is the process of rescaling feature so that they will follow normal distribution",valid:!1,explication:" Of course not. This is called standardization"},{label:"Normalization shrinks the range of  data so that the range is fixed between 0 and 1.",valid:!0}]},{question:"13.\tIn the case of imbalanced data",responses:[{label:"Accuracy is the best metric to use",valid:!1,explication:" It can be very misleading. Althought, we can use confusion matrix, precision, recall, F1:score"},{label:"Decision trees frequently perform well on imbalanced data",valid:!0},{label:"Resampling techniques like oversample minority class or under sample majority class are beneficial in case of imbalanced data",valid:!0,explication:"Warning!! oversampling should be done after data splitting into test and train"},{label:"SMOTE uses KNN to generate synthetic training data",valid:!0}]},{question:"14.\tWhat is true about feature extraction and feature selection",responses:[{label:"Feature selection is choosing some feature based on domain knowledge",valid:!0},{label:"Feature extraction is creating new data from original one",valid:!0},{label:"Feature selection create new subset of new data while feature extraction extract some variables from dataset",valid:!1}]},{question:"15.\twhat is mulicollinearity?",responses:[{label:"It means that two variables are parallel and not related",valid:!1,explication:"It is totally the contrary. it means they arehighly correlated"},{label:" creating new variable from existing one could cause colinearity",valid:!0},{label:"High correlation",valid:!0,explication:"We could discover correlation between two variables with a scatter plot, and strength of correlation with the correlation matrix"},{label:"Reduce multicolinearity by adding more data",valid:!0}]},{question:"16.\tPrincipal component analysis, PCA",responses:[{label:"is a technique of feature selection",valid:!1,explication:"This is a very common error. PCA select some variables from original dataset then create new ones"},{label:"Searches for a linear combination of variables that best separates 2 classes",valid:!0},{label:"It is a supervised algorithm that reduce dimensionality and classify data",valid:!1,explication:"PCA is usupervised and dedicated to dimensionality reduction only"},{label:"Standardization is a must before PCA",valid:!0,explication:"Scaling data is a requirement for the optimal performance of PCA"},{label:"The core of coponent analysis is built on the concept of eigenvectors and eigenvalues",valid:!0,explication:"The technique of Eigenvectors and Eigenvalues are used to compress the data.Eigenvectors are used to make linear transformation understandable. "}]},{question:"17.\tLinear Discriminant analysis",responses:[{label:"It is a way to reduce \u2018dimensionality\u2019 while at the same time preserving as much of the class discrimination information as possible.",valid:!0,explication:"Perfectly right. LDA is a supervised algorithm that takes into consideration the class label"},{label:"Performs better  when we have few samples per class",valid:!1,explication:"PCA is better in this case. Whereas LDA works better with large dataset having multiple classes"},{label:"It finds centroid for each class then projects data points on a line so that clusters are as separated as possible",valid:!0},{label:"Unlike PCA. It makes assumptions about normally distributed classes and equal class covariances",valid:!0}]},{question:"18.\tRegression analysis",responses:[{label:"It's all about finding relationship between dependent variables and independent variables",valid:!0},{label:"Applicable only if the solution is linear",valid:!0},{label:"Independent variables may be continuous or discrete, while dependent variable is discrete",valid:!1,explication:"Independent variables may be continuous or discrete, while dependent variable is continuous"},{label:"Logistic regression is a classification algorithm where dependent variable is binary.",valid:!0},{label:"In the Polynomial Regression, the best fit line is a straight line",valid:!1,explication:"Nope! it can be a curve line which can fit the model perfectly"},{label:"Stepwise regression fits the regression model by adding/dropping predictors on each step based on a specified criterion.",valid:!0,explication:"It aims to maximize the prediction power with minimum number of predictors."}]},{question:"19.\tWhat are regression loss functions?",responses:[{label:"Mean Square Error (L2 loss)",valid:!0,explication:"It is a measure of how close a fitted line is to actual data points."},{label:"Root Mean Square Error",valid:!1},{label:"Mean Absolute Error(L1 loss)",valid:!0,explication:"It measures the magnitude of error without considering their direction. MAE is more robust to outliers since it does not make use of square."},{label:"Huber Loss",valid:!0,explication:"It is less sensitive to outliers in data. it is an Absolute error but when the Error is small then it becomes MSE"},{label:"Log-Cos h Loss",valid:!0,explication:"Log-cosh is the logarithm of the hyperbolic cosine function of the prediction error."}]},{question:"20.\tSigmoid function",responses:[{label:"It takes real value as an input and gives probability which is in between 0 and 1",valid:!0},{label:"It is linear in nature",valid:!1},{label:"It convert high values to 0 and low values to 1",valid:!0}]},{question:"21.\tLogistic regression",responses:[{label:"Linear relationship between dependent and independent variables is required",valid:!1},{label:"The independent variable should not be correlated with each other",valid:!0},{label:"The data is modeled using a straight line",valid:!1,explication:"The probability of events is represented as a linear function of a combination of independent variables"}]},{question:"22.\tKNN classifier",responses:[{label:"assumes that similar things exist in close proximity",valid:!0},{label:"For categorical variables it uses Euclidean, Manhattan or Minkowski distances",valid:!1,explication:" Those distances are used for numerical variables. However with categorical variables, the hamming distance must be used."},{label:"It is an eager learner",valid:!1,explication:"It is a lazy learner. Because it doesn\u2019t learn a discriminative function from the training data but memorizes the training dataset instead."},{label:"It does require a lot of training to find the nearest neighbor",valid:!1,explication:"There is no training time in KNN. Simply because it is lazy :p"},{label:"It doesn't work well when the number of inputs is very large neither with high dimensional data",valid:!0}]},{question:"23.\tNaive bayes (NB)",responses:[{label:"Assumes that all features are independent of each other",valid:!0,explication:" Well that's the secret behind naive adjective. ;)"},{label:"It is a discriminative classifier",valid:!1,explication:"It is a generative classifier. It ?learns the joint probability distribution p (x, y). It predicts the conditional probability with the help of Bayes Theorem. A Discriminative model ?models the decision boundary between the classes. A Discriminative model ?learns the conditional probability distribution p (y |x)."},{label:"Sometimes it become unable to make a prediction",valid:!0,explication:"when the test data contain a categorie which didn't exist in training data"}]},{question:"24.\tTree-based models",responses:[{label:"It uses many trees, and it makes a prediction by averaging the predictions of each component tree. ",valid:!1,explication:"This is random forest. They perform better than one decision tree"},{label:"Calculate the probability of a given record and classify it by assigning it to the most likely class",valid:!0},{label:"It is a parametric model",valid:!1,explication:"It is non parametric model"},{label:"A very good classifier with continuous data",valid:!1,explication:"It performs better with categorical data. But with continuous data it uses the variance to choose the best split with preference to lowest variance."},{label:"Underfitting is when a decision tree fails to capture important distinctions and patterns in the data, so it performs poorly even in training data.",valid:!0,explication:"overfitting is where a model matches the training data almost perfectly, but does poorly in validation and other new data."}]},{question:"25.\tHow to deal with categorical variables",responses:[{label:"In one hot encoding there is no ordering in the categorical data.",valid:!0},{label:"One hot encoding assigns each unique value to a different integer.",valid:!1,explication:"This is label encoding. While one hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data."},{label:"Label encoding replaces each categorical value with the number of times it appears in the dataset",valid:!1,explication:"It is Count encoding."},{label:"Catboost encoding replaces a categorical value with the average value of the target for that value of the feature.",valid:!1,explication:"Wrong. This is called target encoding"},{label:"Target encoding replaces a categorical value with the average value of the target for that value of the feature.",valid:!0}]}],submitted:!1},H=function(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:P,a=arguments.length>1?arguments[1]:void 0;switch(a.type){case f.SET_CHECKBOX:var t=a.payload,i=t.checked,n=t.quizIdx,l=t.optionIdx,o=L()(e);return o.quizes[n].responses[l].checked=i,o;case f.SET_SUBMITTED:var s=a.payload.submitted;return Object(B.a)(Object(B.a)({},e),{},{submitted:s});default:return e}},F=Object(M.c)({quiz:H}),K=Object(M.d)(F,M.a.apply(void 0,[]));o.a.render(n.a.createElement(r.a,{store:K},n.a.createElement(W,null)),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()})).catch((function(e){console.error(e.message)}))}},[[243,1,2]]]);
//# sourceMappingURL=main.5ab49d42.chunk.js.map